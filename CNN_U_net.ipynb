{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDColor Image Colorization Training\n",
    "\n",
    "Training pipeline for DDColor model. See README.md for detailed architecture and theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Installation\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio\n",
    "pip install opencv-python pillow numpy pyyaml scipy scikit-image timm tensorboard\n",
    "pip install lmdb lpips\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import urllib.request\n",
    "\n",
    "from ddcolor_model import DDColor\n",
    "from basicsr.losses import PerceptualLoss, GANLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Paths - modify these for your setup\n",
    "    train_data_dir = './dataset/train'\n",
    "    val_data_dir = './dataset/test'\n",
    "    save_dir = './experiments/ddcolor_custom'\n",
    "    pretrain_dir = './pretrain'\n",
    "    \n",
    "    # Model architecture\n",
    "    encoder_name = 'convnext-l'\n",
    "    decoder_name = 'MultiScaleColorDecoder'\n",
    "    num_queries = 100\n",
    "    num_scales = 3\n",
    "    dec_layers = 9\n",
    "    input_size = 512\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size = 4\n",
    "    num_workers = 4\n",
    "    num_epochs = 100\n",
    "    total_iters = 400000\n",
    "    \n",
    "    # Optimizer\n",
    "    lr = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    betas = (0.9, 0.99)\n",
    "    \n",
    "    # Loss weights\n",
    "    lambda_pixel = 0.1\n",
    "    lambda_perceptual = 5.0\n",
    "    lambda_gan = 1.0\n",
    "    lambda_colorfulness = 0.5\n",
    "    \n",
    "    # Logging frequencies\n",
    "    print_freq = 100\n",
    "    save_freq = 5000\n",
    "    val_freq = 2000\n",
    "    \n",
    "    # Device configuration\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    use_amp = torch.cuda.is_available()\n",
    "    \n",
    "    # Multi-GPU settings\n",
    "    use_distributed = False\n",
    "    gpu_ids = [0]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Create necessary directories\n",
    "for directory in [config.save_dir, config.pretrain_dir, \n",
    "                  f'{config.save_dir}/checkpoints', f'{config.save_dir}/samples']:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "print(f\"Configuration loaded\")\n",
    "print(f\"Device: {config.device}\")\n",
    "print(f\"Mixed precision: {config.use_amp}\")\n",
    "print(f\"Training data: {config.train_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pretrained_weights():\n",
    "    weights = {\n",
    "        'convnext': {\n",
    "            'url': 'https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth',\n",
    "            'path': f'{config.pretrain_dir}/convnext_large_22k_224.pth'\n",
    "        },\n",
    "        'inception': {\n",
    "            'url': 'https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth',\n",
    "            'path': f'{config.pretrain_dir}/inception_v3_google-1a9a5a14.pth'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for name, info in weights.items():\n",
    "        if not os.path.exists(info['path']):\n",
    "            print(f\"Downloading {name} weights...\")\n",
    "            urllib.request.urlretrieve(info['url'], info['path'])\n",
    "            print(f\"Downloaded: {info['path']}\")\n",
    "        else:\n",
    "            print(f\"{name} weights found: {info['path']}\")\n",
    "\n",
    "download_pretrained_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizeDataset(Dataset):\n",
    "    def __init__(self, image_dir, input_size=256, is_train=True):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.input_size = input_size\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        self.image_paths = []\n",
    "        for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
    "            self.image_paths.extend(list(self.image_dir.glob(ext)))\n",
    "        \n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {image_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images in {image_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def rgb_to_lab(self, img):\n",
    "        img_lab = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2LAB)\n",
    "        return img_lab.astype(np.float32) / 255.0\n",
    "    \n",
    "    def lab_to_rgb(self, img_lab):\n",
    "        img_lab = (img_lab * 255).astype(np.uint8)\n",
    "        img_rgb = cv2.cvtColor(img_lab, cv2.COLOR_LAB2RGB)\n",
    "        return img_rgb.astype(np.float32) / 255.0\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        img = img.resize((self.input_size, self.input_size), Image.BICUBIC)\n",
    "        img = np.array(img).astype(np.float32) / 255.0\n",
    "        \n",
    "        img_lab = self.rgb_to_lab(img)\n",
    "        img_l = img_lab[:, :, 0:1]\n",
    "        img_ab = img_lab[:, :, 1:3]\n",
    "        \n",
    "        img_l = torch.from_numpy(img_l.transpose(2, 0, 1)).float()\n",
    "        img_ab = torch.from_numpy(img_ab.transpose(2, 0, 1)).float()\n",
    "        \n",
    "        # Create grayscale RGB for model input\n",
    "        img_gray_lab = np.concatenate([img_lab[:, :, 0:1], \n",
    "                                        np.zeros_like(img_l.numpy().transpose(1, 2, 0)),\n",
    "                                        np.zeros_like(img_l.numpy().transpose(1, 2, 0))], axis=-1)\n",
    "        img_gray_rgb = self.lab_to_rgb(img_gray_lab)\n",
    "        img_gray_rgb = torch.from_numpy(img_gray_rgb.transpose(2, 0, 1)).float()\n",
    "        \n",
    "        return {\n",
    "            'gray_rgb': img_gray_rgb,\n",
    "            'l': img_l,\n",
    "            'ab': img_ab,\n",
    "            'rgb': torch.from_numpy(img.transpose(2, 0, 1)).float()\n",
    "        }\n",
    "\n",
    "train_dataset = ColorizeDataset(config.train_data_dir, config.input_size, is_train=True)\n",
    "val_dataset = ColorizeDataset(config.val_data_dir, config.input_size, is_train=False)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Dataset loaded: {len(train_dataset)} train, {len(val_dataset)} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorfulnessLoss(nn.Module):\n",
    "    \"\"\"Encourages vibrant, saturated colors\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, ab_pred):\n",
    "        std = torch.std(ab_pred, dim=[2, 3])\n",
    "        mean = torch.mean(torch.abs(ab_pred), dim=[2, 3])\n",
    "        colorfulness = torch.mean(std + 0.3 * mean)\n",
    "        return 1.0 / (colorfulness + 1e-8)\n",
    "\n",
    "class PixelLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss_fn = nn.L1Loss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        return self.loss_fn(pred, target)\n",
    "\n",
    "pixel_loss = PixelLoss().to(config.device)\n",
    "perceptual_loss = PerceptualLoss(\n",
    "    layer_weights={'conv5_4': 1.0},\n",
    "    vgg_type='vgg19',\n",
    "    use_input_norm=True,\n",
    "    perceptual_weight=1.0,\n",
    "    style_weight=0,\n",
    "    criterion='l1'\n",
    ").to(config.device)\n",
    "colorfulness_loss = ColorfulnessLoss().to(config.device)\n",
    "\n",
    "print(\"Loss functions initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    \"\"\"PatchGAN discriminator for adversarial training\"\"\"\n",
    "    def __init__(self, input_nc=3, ndf=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, kernel_size=4, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=1, padding=1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "discriminator = PatchDiscriminator().to(config.device)\n",
    "gan_loss = GANLoss(gan_type='vanilla', real_label_val=1.0, fake_label_val=0.0).to(config.device)\n",
    "\n",
    "print(\"Discriminator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDColor(\n",
    "    encoder_name=config.encoder_name,\n",
    "    decoder_name=config.decoder_name,\n",
    "    input_size=[config.input_size, config.input_size],\n",
    "    num_output_channels=2,\n",
    "    last_norm='Spectral',\n",
    "    do_normalize=False,\n",
    "    num_queries=config.num_queries,\n",
    "    num_scales=config.num_scales,\n",
    "    dec_layers=config.dec_layers,\n",
    ").to(config.device)\n",
    "\n",
    "# Load pretrained encoder\n",
    "print(\"Loading pretrained encoder...\")\n",
    "try:\n",
    "    pretrained_dict = torch.load(\n",
    "        f'{config.pretrain_dir}/convnext_large_22k_224.pth',\n",
    "        map_location=config.device\n",
    "    )\n",
    "    model_dict = model.state_dict()\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    model_dict.update(pretrained_dict)\n",
    "    model.load_state_dict(model_dict, strict=False)\n",
    "    print(\"Pretrained weights loaded\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not load pretrained weights ({e})\")\n",
    "    print(\"Training from scratch\")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()) / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers & Schedulers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_g = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config.lr,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=config.betas\n",
    ")\n",
    "\n",
    "optimizer_d = optim.AdamW(\n",
    "    discriminator.parameters(),\n",
    "    lr=config.lr,\n",
    "    weight_decay=config.weight_decay,\n",
    "    betas=config.betas\n",
    ")\n",
    "\n",
    "scheduler_g = optim.lr_scheduler.StepLR(optimizer_g, step_size=80000, gamma=0.5)\n",
    "scheduler_d = optim.lr_scheduler.StepLR(optimizer_d, step_size=80000, gamma=0.5)\n",
    "\n",
    "scaler = GradScaler() if config.use_amp else None\n",
    "\n",
    "print(\"Optimizers initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lab_to_rgb_tensor(l, ab):\n",
    "    \"\"\"Convert LAB tensors to RGB for visualization\"\"\"\n",
    "    lab = torch.cat([l, ab], dim=1)\n",
    "    lab_np = lab.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    \n",
    "    rgb_list = []\n",
    "    for i in range(lab_np.shape[0]):\n",
    "        lab_img = (lab_np[i] * 255).astype(np.uint8)\n",
    "        rgb_img = cv2.cvtColor(lab_img, cv2.COLOR_LAB2RGB)\n",
    "        rgb_list.append(rgb_img)\n",
    "    \n",
    "    rgb_np = np.stack(rgb_list, axis=0)\n",
    "    rgb_tensor = torch.from_numpy(rgb_np.transpose(0, 3, 1, 2)).float() / 255.0\n",
    "    return rgb_tensor.to(l.device)\n",
    "\n",
    "def train_one_epoch(epoch, iteration):\n",
    "    model.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        iteration += 1\n",
    "        \n",
    "        gray_rgb = batch['gray_rgb'].to(config.device)\n",
    "        l_gt = batch['l'].to(config.device)\n",
    "        ab_gt = batch['ab'].to(config.device)\n",
    "        \n",
    "        # Train generator\n",
    "        optimizer_g.zero_grad()\n",
    "        \n",
    "        if config.use_amp:\n",
    "            with autocast(device_type=config.device):\n",
    "                ab_pred = model(gray_rgb)\n",
    "                \n",
    "                loss_pixel = pixel_loss(ab_pred, ab_gt) * config.lambda_pixel\n",
    "                \n",
    "                rgb_pred = lab_to_rgb_tensor(l_gt, ab_pred)\n",
    "                rgb_gt = lab_to_rgb_tensor(l_gt, ab_gt)\n",
    "                loss_perceptual = perceptual_loss(rgb_pred, rgb_gt)[0] * config.lambda_perceptual\n",
    "                \n",
    "                loss_color = colorfulness_loss(ab_pred) * config.lambda_colorfulness\n",
    "                \n",
    "                fake_pred = discriminator(rgb_pred)\n",
    "                loss_gan_g = gan_loss(fake_pred, True) * config.lambda_gan\n",
    "                \n",
    "                loss_g = loss_pixel + loss_perceptual + loss_color + loss_gan_g\n",
    "            \n",
    "            scaler.scale(loss_g).backward()\n",
    "            scaler.step(optimizer_g)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            ab_pred = model(gray_rgb)\n",
    "            loss_pixel = pixel_loss(ab_pred, ab_gt) * config.lambda_pixel\n",
    "            rgb_pred = lab_to_rgb_tensor(l_gt, ab_pred)\n",
    "            rgb_gt = lab_to_rgb_tensor(l_gt, ab_gt)\n",
    "            loss_perceptual = perceptual_loss(rgb_pred, rgb_gt)[0] * config.lambda_perceptual\n",
    "            loss_color = colorfulness_loss(ab_pred) * config.lambda_colorfulness\n",
    "            fake_pred = discriminator(rgb_pred)\n",
    "            loss_gan_g = gan_loss(fake_pred, True) * config.lambda_gan\n",
    "            loss_g = loss_pixel + loss_perceptual + loss_color + loss_gan_g\n",
    "            \n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "        \n",
    "        # Train discriminator\n",
    "        optimizer_d.zero_grad()\n",
    "        \n",
    "        if config.use_amp:\n",
    "            with autocast(device_type=config.device):\n",
    "                real_pred = discriminator(rgb_gt.detach())\n",
    "                loss_d_real = gan_loss(real_pred, True)\n",
    "                \n",
    "                fake_pred = discriminator(rgb_pred.detach())\n",
    "                loss_d_fake = gan_loss(fake_pred, False)\n",
    "                \n",
    "                loss_d = (loss_d_real + loss_d_fake) * 0.5\n",
    "            \n",
    "            scaler.scale(loss_d).backward()\n",
    "            scaler.step(optimizer_d)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            real_pred = discriminator(rgb_gt.detach())\n",
    "            loss_d_real = gan_loss(real_pred, True)\n",
    "            fake_pred = discriminator(rgb_pred.detach())\n",
    "            loss_d_fake = gan_loss(fake_pred, False)\n",
    "            loss_d = (loss_d_real + loss_d_fake) * 0.5\n",
    "            \n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "        \n",
    "        scheduler_g.step()\n",
    "        scheduler_d.step()\n",
    "        \n",
    "        if iteration % config.print_freq == 0:\n",
    "            pbar.set_postfix({\n",
    "                'iter': iteration,\n",
    "                'L_pix': f'{loss_pixel.item():.4f}',\n",
    "                'L_per': f'{loss_perceptual.item():.4f}',\n",
    "                'L_col': f'{loss_color.item():.4f}',\n",
    "                'L_gan': f'{loss_gan_g.item():.4f}',\n",
    "                'L_d': f'{loss_d.item():.4f}',\n",
    "            })\n",
    "        \n",
    "        if iteration % config.val_freq == 0:\n",
    "            save_samples(epoch, iteration)\n",
    "        \n",
    "        if iteration % config.save_freq == 0:\n",
    "            save_checkpoint(epoch, iteration)\n",
    "        \n",
    "        if iteration >= config.total_iters:\n",
    "            return iteration\n",
    "    \n",
    "    return iteration\n",
    "\n",
    "def save_samples(epoch, iteration):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch = next(iter(val_loader))\n",
    "        gray_rgb = batch['gray_rgb'].to(config.device)\n",
    "        l_gt = batch['l'].to(config.device)\n",
    "        ab_gt = batch['ab'].to(config.device)\n",
    "        \n",
    "        ab_pred = model(gray_rgb)\n",
    "        rgb_pred = lab_to_rgb_tensor(l_gt, ab_pred)\n",
    "        rgb_gt = lab_to_rgb_tensor(l_gt, ab_gt)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        axes[0].imshow(gray_rgb[0].cpu().permute(1, 2, 0))\n",
    "        axes[0].set_title('Input')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(rgb_pred[0].cpu().permute(1, 2, 0))\n",
    "        axes[1].set_title('Predicted')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(rgb_gt[0].cpu().permute(1, 2, 0))\n",
    "        axes[2].set_title('Ground Truth')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{config.save_dir}/samples/epoch{epoch}_iter{iteration}.png')\n",
    "        plt.close()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "def save_checkpoint(epoch, iteration):\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'iteration': iteration,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'discriminator_state_dict': discriminator.state_dict(),\n",
    "        'optimizer_g_state_dict': optimizer_g.state_dict(),\n",
    "        'optimizer_d_state_dict': optimizer_d.state_dict(),\n",
    "        'scheduler_g_state_dict': scheduler_g.state_dict(),\n",
    "        'scheduler_d_state_dict': scheduler_d.state_dict(),\n",
    "    }\n",
    "    \n",
    "    path = f'{config.save_dir}/checkpoints/checkpoint_iter{iteration}.pth'\n",
    "    torch.save(checkpoint, path)\n",
    "    print(f\"\\nCheckpoint saved: {path}\")\n",
    "    \n",
    "    latest_path = f'{config.save_dir}/checkpoints/latest.pth'\n",
    "    shutil.copy(path, latest_path)\n",
    "\n",
    "print(\"Training functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Epochs: {config.num_epochs}\")\n",
    "    print(f\"Iterations: {config.total_iters}\")\n",
    "    print(f\"Batch size: {config.batch_size}\")\n",
    "    print(f\"Device: {config.device}\")\n",
    "    print(f\"Output: {config.save_dir}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"EPOCH {epoch + 1}/{config.num_epochs}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        iteration = train_one_epoch(epoch, iteration)\n",
    "        \n",
    "        if iteration >= config.total_iters:\n",
    "            print(f\"\\nReached maximum iterations: {config.total_iters}\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Final checkpoint: {config.save_dir}/checkpoints/latest.pth\")\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(image_path, checkpoint_path):\n",
    "    \"\"\"Colorize a grayscale image\"\"\"\n",
    "    model = DDColor(\n",
    "        encoder_name=config.encoder_name,\n",
    "        decoder_name=config.decoder_name,\n",
    "        input_size=[config.input_size, config.input_size],\n",
    "        num_output_channels=2,\n",
    "        last_norm='Spectral',\n",
    "        do_normalize=False,\n",
    "        num_queries=config.num_queries,\n",
    "        num_scales=config.num_scales,\n",
    "        dec_layers=config.dec_layers,\n",
    "    ).to(config.device)\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=config.device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img = img.resize((config.input_size, config.input_size))\n",
    "    img = np.array(img).astype(np.float32) / 255.0\n",
    "    \n",
    "    img_lab = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2LAB).astype(np.float32) / 255.0\n",
    "    img_l = img_lab[:, :, 0:1]\n",
    "    \n",
    "    img_gray_lab = np.concatenate([\n",
    "        img_lab[:, :, 0:1],\n",
    "        np.zeros_like(img_l),\n",
    "        np.zeros_like(img_l)\n",
    "    ], axis=-1)\n",
    "    img_gray_rgb = cv2.cvtColor((img_gray_lab * 255).astype(np.uint8), cv2.COLOR_LAB2RGB).astype(np.float32) / 255.0\n",
    "    img_gray_rgb = torch.from_numpy(img_gray_rgb.transpose(2, 0, 1)).float().unsqueeze(0).to(config.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ab_pred = model(img_gray_rgb)\n",
    "    \n",
    "    l_tensor = torch.from_numpy(img_l.transpose(2, 0, 1)).float().unsqueeze(0).to(config.device)\n",
    "    rgb_pred = lab_to_rgb_tensor(l_tensor, ab_pred)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].imshow(img_gray_rgb[0].cpu().permute(1, 2, 0))\n",
    "    axes[0].set_title('Input')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(rgb_pred[0].cpu().permute(1, 2, 0))\n",
    "    axes[1].set_title('Colorized')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return rgb_pred\n",
    "\n",
    "# Example usage\n",
    "# result = inference('./test_image.jpg', f'{config.save_dir}/checkpoints/latest.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}